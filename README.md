# BERT-Knowledge-Distillation
Research on attention-based model compression for NLP
